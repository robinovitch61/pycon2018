{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Non-Parametric Models for Data Science with PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* statistical assumptions determine model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* linear relationships among variables are often inappropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "automated fitting in pymc3 discussed using bayesian methods (probabilistic programming)\n",
    "* make inferences about things we care about using probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modeling complex non-linear functions using gaussian distributions/normal distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Distributions;\n",
    "* conditioning property: conditional distribution of some elements of a multivariate normal is also normal (closed form calculations)\n",
    "* marginalization property: marginal distribution of some elements of amultivariate noraml is also normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian process:\n",
    "* infinite collection of random variables, any finite subset of which have a gaussian distribution\n",
    "* non-parametric really means infinite number of parameters\n",
    "  * number of parameters we use scales with volume of data\n",
    "* modeling underlying distribution directly\n",
    "* thing that makes it go: covariance function (generates covariance matrices given some inputs)\n",
    "* mean functions -- generate mean vectors\n",
    "  * easily spec'd as 0 or C, doesn't influence much\n",
    "* gaussian process fully spec'd by covariance and mean functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prior processes by specifying something like GP ~ mean=0, std~1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian processes:\n",
    "* GPy\n",
    "* GPflow\n",
    "* scikit-learn\n",
    "* pystan\n",
    "* pymc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BTW he wrote pymc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYMC3:\n",
    "* Currently based on Theano but moving to Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizations of GP's are functions, not values -- he drew 3, got 3 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian -- since everything in terms of prob, everything has honest uncertainty bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal likelihood takes into account overfitting -- regularization taken care of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get distributions of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations:\n",
    "* Not for big data (doesn't scale very well)\n",
    "* Problem: posterior covariance function, must invert large matrix -- O(N^3) in compute time, O(N^2) in memory\n",
    "  * Work around: select subset of data M << N, sparse approximation methods\n",
    "    * GP as MarginalSparse, give approximation method (FITC)\n",
    "      * PYMC3 uses K-Means to determine where the subset is selected from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multidimensional fits are possible, use sparse marginal models for large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GP for:\n",
    "* Classification\n",
    "* Automatic relevance determination\n",
    "* NN are special case\n",
    "* Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "* Mindless way of doing nonlinear regression -- recover arbitrary nonlinear functions with probabilistic bounds\n",
    "* Interpretable hyperparameters\n",
    "* Easy to automate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bill Engels did pymc3 in one summer...badass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
