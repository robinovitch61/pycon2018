{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark binary communicates with JVM (java virtual machine) -- this is required for install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google \"Pyspark dataframe api, pyspark api\": http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "  * pyspark.sqlmodule has almost all the interesting stuff:\n",
    "    * communications with pandas, pyspark sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object you care about when interacting with pyspark: SparkSession (usually named \"spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession contains info about communication with cluster -- config info (how many cores, executors, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSQL runs Hive flavour of SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.createDataFrame() creates dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more typical: spark.read.parquet (returns spark df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spark, SQL and df's are the same thing (easy to go back and forth bn dataframes and things you can call from SQL)\n",
    "  * df.create/replaceTempView(NAME) --> df's data now available in spark.SQL with tablename NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark.sql functions, you can pass df's into --> these run in scala, makes things fast\n",
    "  * also have highly optimized sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips:\n",
    "* Stay in JVM --> always faster, sql optimization\n",
    "* Use good data format (parquet way better than JSON)\n",
    "  * Parquet is a file format optimized for hadoop, good for sql\n",
    "* If using python, use pandas_udf --> new in spark 2.3, https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\n",
    "  * Runs 10x20 faster\n",
    "  * Must have pyarrow, uses apache arrow --> arrow helps translate from arrow to python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASK ABOUT DYNAMIC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
